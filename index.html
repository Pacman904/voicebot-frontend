<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>OpenAI VoiceBot</title>
  <style>
    body { 
      font-family: Arial; 
      display: flex; 
      justify-content: center; 
      align-items: center; 
      height: 100vh; 
      background: #f4f4f4; 
    }
    .container { 
      background: white; 
      padding: 20px; 
      border-radius: 8px; 
      box-shadow: 0 2px 8px rgba(0,0,0,0.1); 
    }
    button { 
      padding: 12px 24px; 
      background: #007bff; 
      color: white; 
      border: none; 
      border-radius: 4px; 
      cursor: pointer; 
    }
    #onAir { 
      margin-top: 15px; 
      font-size: 20px; 
      color: green; 
      display: none; 
    }
    audio { 
      width: 100%; 
      margin: 15px 0; 
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>OpenAI VoiceBot</h1>
    <button id="startButton">Verbindung starten</button>
    <div id="onAir">Verbunden!</div>
    <audio id="openAIAudio" controls></audio>
    <audio id="userAudio" controls></audio>
  </div>
  <script src="https://unpkg.com/parse/dist/parse.min.js"></script>
  <script>
    Parse.initialize("uNEbQKjymSX8qmK9gorQNRbSuaV23eMHriF2Yeoi", "NY08Fa7grB4I1AOOcrjKN2b9w6sWke5GCA8rpxBM");
    Parse.serverURL = "https://parseapi.back4app.com/";

    let peerConnection = null;
    let userStream = null;
    let dc = null;
    let audioBuffer = [];
    let isSpeaking = false;

    async function initializeWebRTC(iceServers, clientSecret, parseSessionId) {
      const configuration = {
        iceServers: iceServers || [
          { urls: "stun:stun.l.google.com:19302" },
          { 
            urls: "turn:numb.viagenie.ca",
            username: "webrtc@live.com",
            credential: "muazkh" 
          }
        ]
      };

      peerConnection = new RTCPeerConnection(configuration);

      // ICE-Kandidaten senden
      peerConnection.onicecandidate = async (event) => {
        if (event.candidate) {
          await sendSignalingData(parseSessionId, {
            type: "candidate",
            candidate: event.candidate.candidate,
            sdpMid: event.candidate.sdpMid,
            sdpMLineIndex: event.candidate.sdpMLineIndex
          });
        }
      };

      // OpenAI-Audio empfangen
      peerConnection.ontrack = (event) => {
        const audioEl = document.getElementById('openAIAudio');
        audioEl.srcObject = event.streams[0];
      };

      // Data-Channel für Kontroll-Ereignisse
      dc = peerConnection.createDataChannel("oaiControlEvents");
      dc.onopen = () => {
        console.log("Data-Channel verbunden");
        //珊瑚修正：Starte Audio-Verarbeitung
        startAudioProcessing();
      };
      dc.onmessage = (e) => {
        const event = JSON.parse(e.data);
        console.log("OpenAI Event:", event);
        if (event.type === "error") {
          console.error("OpenAI Session-Fehler:", event.error);
        }
      };

      return peerConnection;
    }

    async function startConnection() {
      try {
        const response = await Parse.Cloud.run("createEphemeralSession", {
          model: "gpt-4o-mini-realtime-preview-2024-12-17",
          voice: "verse"
        });

        userStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        document.getElementById('userAudio').srcObject = userStream;
        
        peerConnection = await initializeWebRTC(
          response.iceServers, 
          response.clientSecret, 
          response.sessionId
        );

        //珊瑚修正：Teile Audio-Track
        userStream.getAudioTracks().forEach(track => peerConnection.addTrack(track));

        await handleSignaling(response.sessionId, response.clientSecret);
      } catch (error) {
        alert(error.message);
      }
    }

    async function handleSignaling(sessionId, clientSecret) {
      const offer = await peerConnection.createOffer({ offerToReceiveAudio: true });
      await peerConnection.setLocalDescription(offer);

      await Parse.Cloud.run("sendSignalingData", {
        sessionId: sessionId,
        data: { type: "offer", sdp: offer.sdp }
      });

      const sdpResponse = await fetch(`https://api.openai.com/v1/realtime?model=gpt-4o-mini-realtime-preview-2024-12-17`, {
        method: "POST",
        body: offer.sdp,
        headers: {
          Authorization: `Bearer ${clientSecret}`,
          "Content-Type": "application/sdp"
        }
      });

      if (!sdpResponse.ok) {
        const errorData = await sdpResponse.json();
        throw new Error(`OpenAI-Fehler: ${errorData.error?.message}`);
      }

      const answerSdp = await sdpResponse.text();
      await peerConnection.setRemoteDescription({ type: "answer", sdp: answerSdp });

      const signalingData = await Parse.Cloud.run("getSignalingData", { sessionId });
      signalingData.forEach(data => {
        if (data.type === "candidate") {
          const candidate = new RTCIceCandidate({
            candidate: data.candidate,
            sdpMid: data.sdpMid,
            sdpMLineIndex: data.sdpMLineIndex
          });
          peerConnection.addIceCandidate(candidate);
        }
      });
    }

    function startAudioProcessing() {
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(userStream);
      const processor = audioContext.createScriptProcessor(4096, 1, 1);

      source.connect(processor);
      processor.connect(audioContext.destination);

      processor.onaudioprocess = (event) => {
        const buffer = event.inputBuffer.getChannelData(0);
        const volume = Math.sqrt(buffer.reduce((sum, sample) => sum + sample ** 2, 0) / buffer.length);

        if (volume > 0.1) {
          isSpeaking = true;
          audioBuffer.push(buffer);
        } else {
          if (isSpeaking) {
            setTimeout(() => {
              if (!isSpeaking) {
                sendAudioBuffer();
              }
            }, 1000); //珊瑚修正：Warte 1 Sekunde bis zum Commit
          }
        }
      };

      processor.onaudioprocess = (event) => {
        const buffer = event.inputBuffer.getChannelData(0);
        audioBuffer.push(buffer);
        if (isSpeaking) {
          dc.send(JSON.stringify({ type: "input_audio_buffer.append", audio: audioBuffer }));
          audioBuffer = [];
        }
      };

      userStream.getAudioTracks()[0].onended = () => {
        if (isSpeaking) {
          sendAudioBuffer();
        }
      };
    }

    function sendAudioBuffer() {
      dc.send(JSON.stringify({ type: "input_audio_buffer.commit" }));
      isSpeaking = false;
      audioBuffer = [];
    }

    async function sendSignalingData(sessionId, data) {
      await Parse.Cloud.run("sendSignalingData", {
        sessionId: sessionId,
        data: data
      });
    }

    document.getElementById('startButton').addEventListener('click', startConnection);

    window.addEventListener('beforeunload', () => {
      if (peerConnection) peerConnection.close();
      if (userStream) userStream.getTracks().forEach(track => track.stop());
    });
  </script>
</body>
</html>
