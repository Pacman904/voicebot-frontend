<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenAI Voice Assistant</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            background-color: #f4f4f4;
            margin: 0;
        }
        .container {
            display: flex;
            align-items: center;
            gap: 20px;
        }
        .phone {
            width: 300px;
            height: 600px;
            background-color: #111;
            border-radius: 30px;
            padding: 20px;
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
            position: relative;
            overflow: hidden;
        }
        .phone-screen {
            background-color: white;
            height: 100%;
            border-radius: 20px;
            overflow: hidden;
            display: flex;
            flex-direction: column;
        }
        .phone-status {
            height: 30px;
            background-color: #f8f8f8;
            border-bottom: 1px solid #ddd;
            display: flex;
            align-items: center;
            padding: 0 15px;
        }
        .call-status {
            font-size: 14px;
            font-weight: bold;
            color: #4CAF50;
            display: none;
        }
        .phone-content {
            flex: 1;
            padding: 15px;
            overflow-y: auto;
            background-color: #f9f9f9;
        }
        .message-box {
            background-color: white;
            border-radius: 10px;
            padding: 10px;
            margin-bottom: 10px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
            word-wrap: break-word;
        }
        .phone-bottom {
            height: 80px;
            background-color: #f8f8f8;
            display: flex;
            justify-content: center;
            align-items: center;
            border-top: 1px solid #ddd;
        }
        .call-button {
            width: 60px;
            height: 60px;
            border-radius: 50%;
            background-color: #4CAF50;
            display: flex;
            justify-content: center;
            align-items: center;
            cursor: pointer;
            transition: background-color 0.3s;
        }
        .call-button.active {
            background-color: #f44336;
        }
        .call-button i {
            color: white;
            font-size: 24px;
        }
        .speech-bubble {
            position: relative;
            background: #ffffff;
            border-radius: 15px;
            padding: 20px;
            width: 250px;
            min-height: 100px;
            max-height: 400px;
            overflow-y: auto;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            display: none;
        }
        .speech-bubble:after {
            content: '';
            position: absolute;
            left: 0;
            top: 30px;
            width: 0;
            height: 0;
            border: 15px solid transparent;
            border-right-color: #ffffff;
            border-left: 0;
            margin-top: -15px;
            margin-left: -15px;
        }
        .error {
            color: #f44336;
            font-weight: bold;
        }
        audio {
            display: none;
        }
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
</head>
<body>
    <div class="container">
        <div class="phone">
            <div class="phone-screen">
                <div class="phone-status">
                    <div id="callStatus" class="call-status">Active Call</div>
                </div>
                <div id="phoneContent" class="phone-content">
                    <div class="message-box">
                        Ready to connect to OpenAI voice assistant. Press the green button to start.
                    </div>
                </div>
                <div class="phone-bottom">
                    <div id="callButton" class="call-button">
                        <i class="fas fa-phone"></i>
                    </div>
                </div>
            </div>
        </div>
        <div id="speechBubble" class="speech-bubble">
            <p>OpenAI responses will appear here once connected.</p>
        </div>
    </div>
    <script src="https://unpkg.com/parse/dist/parse.min.js"></script>
    <script>
        Parse.initialize("uNEbQKjymSX8qmK9gorQNRbSuaV23eMHriF2Yeoi", "NY08Fa7grB4I1AOOcrjKN2b9w6sWke5GCA8rpxBM");
        Parse.serverURL = "https://parseapi.back4app.com";

        let peerConnection = null;
        let dataChannel = null;
        let isCallActive = false;
        let sessionId = null;
        let audioContext = null;
        let mediaStream = null;
        let audioElement = null;
        let isAudioContextResumed = false;
        let openAIModel = "gpt-4o-realtime-preview-2024-12-17";
        let audioStreamTrack = null;
        let clientSecret = null;

        const callButton = document.getElementById('callButton');
        const phoneContent = document.getElementById('phoneContent');
        const callStatus = document.getElementById('callStatus');
        const speechBubble = document.getElementById('speechBubble');

        function addMessage(message, isError = false) {
            const messageBox = document.createElement('div');
            messageBox.className = 'message-box';
            if (isError) messageBox.classList.add('error');
            messageBox.textContent = message;
            phoneContent.appendChild(messageBox);
            phoneContent.scrollTop = phoneContent.scrollHeight;
        }

        function updateSpeechBubble(text) {
            speechBubble.innerHTML = `<p>${text}</p>`;
            speechBubble.style.display = 'block';
        }

        function initializeWebRTC(iceServers) {
            try {
                const configuration = { iceServers };
                peerConnection = new RTCPeerConnection(configuration);
                
                peerConnection.onicecandidate = async (event) => {
                    if (event.candidate) {
                        console.log("ICE Candidate gathered:", event.candidate);
                    }
                };

                peerConnection.onconnectionstatechange = () => {
                    console.log("Connection state changed to:", peerConnection.connectionState);
                    if (peerConnection.connectionState === "connected") {
                        callStatus.style.display = "block";
                        addMessage("Connection established successfully!");
                        speechBubble.style.display = 'block';
                        
                        if (audioContext && audioContext.state === 'suspended') {
                            audioContext.resume().then(() => {
                                console.log("Audio context resumed");
                                isAudioContextResumed = true;
                            }).catch(error => {
                                console.error("Failed to resume audio context:", error);
                            });
                        }
                    } else if (peerConnection.connectionState === "disconnected" ||
                               peerConnection.connectionState === "failed") {
                        endCall();
                        addMessage("Connection lost or failed", true);
                    }
                };

                peerConnection.ontrack = (event) => {
                    console.log("Audio Track received:", event.track);
                    
                    if (!audioElement) {
                        audioElement = document.createElement("audio");
                        audioElement.autoplay = true;
                        audioElement.volume = 1.0;
                        document.body.appendChild(audioElement);
                    }
                    
                    audioElement.srcObject = event.streams[0];
                };

                dataChannel = peerConnection.createDataChannel("oai-events");
                dataChannel.onopen = () => {
                    console.log("Data channel opened");
                    addMessage("Communication channel opened");
                };
                
                dataChannel.onmessage = (e) => {
                    try {
                        const event = JSON.parse(e.data);
                        console.log("OpenAI Realtime Event:", event);
                        
                        if (event.type === "transcript") {
                            updateSpeechBubble(event.text);
                        } else if (event.type === "error") {
                            const errorMessage = event.error?.message || event.error?.param || event.message || "Unknown error";
                            addMessage(`OpenAI Error: ${errorMessage}`, true);
                            endCall();
                        } else if (event.type === "status") {
                            addMessage(`Status: ${event.text}`);
                        } else if (event.type === "response") {
                            if (event.output && event.output[0] && event.output[0].audio) {
                                const audioUrl = event.output[0].audio.url;
                                if (audioElement) {
                                    audioElement.src = audioUrl;
                                    audioElement.play().catch(error => {
                                        console.error("Error playing audio:", error);
                                        addMessage("Failed to play audio response", true);
                                    });
                                }
                            }
                        } else if (event.type === "response.done") {
                            if (event.response.status === "failed") {
                                addMessage(`Response failed: ${event.response.status_details.error.message}`, true);
                            } else {
                                addMessage("Response completed successfully");
                            }
                        }
                    } catch (error) {
                        console.error("Error parsing message:", error);
                        addMessage(`Error parsing message: ${error.message}`, true);
                    }
                };

                return peerConnection;
            } catch (error) {
                console.error("WebRTC initialization error:", error);
                addMessage(`WebRTC initialization error: ${error.message}`, true);
                throw error;
            }
        }

        async function startUserMedia() {
            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
                addMessage("Microphone access granted");
                
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                
                mediaStream.getTracks().forEach(track => {
                    if (track.kind === 'audio') {
                        audioStreamTrack = track;
                        peerConnection.addTrack(track, mediaStream);
                    }
                });
                
                return mediaStream;
            } catch (error) {
                console.error("Media access error:", error);
                addMessage(`Microphone access error: ${error.message}`, true);
                throw error;
            }
        }

        async function startConnection() {
            try {
                addMessage("Initializing connection...");
                
                const response = await Parse.Cloud.run("createEphemeralSession", {
                    model: openAIModel,
                    voice: "alloy"
                });

                if (!response || typeof response !== 'object' || !response.sessionId || !response.clientSecret) {
                    addMessage("Response dump: " + JSON.stringify(response), true);
                    throw new Error("Invalid server response: missing required fields");
                }
                
                sessionId = response.sessionId;
                clientSecret = response.clientSecret;

                initializeWebRTC(response.iceServers);
                await startUserMedia();

                // Create and set local description
                const offer = await peerConnection.createOffer({
                    offerToReceiveAudio: true,
                    offerToReceiveVideo: false
                });
                await peerConnection.setLocalDescription(offer);

                // Connect to OpenAI Realtime API using the clientSecret
                const sdpResponse = await fetch("https://api.openai.com/v1/realtime/sessions", {
                    method: "POST",
                    headers: {
                        "Authorization": `Bearer ${clientSecret}`,
                        "Content-Type": "application/json"
                    },
                    body: JSON.stringify({
                        sdp: offer.sdp,
                        model: openAIModel
                    })
                });

                if (!sdpResponse.ok) {
                    const errorText = await sdpResponse.text();
                    throw new Error(`OpenAI Error: ${errorText}`);
                }

                const answerData = await sdpResponse.json();
                const answer = {
                    type: "answer",
                    sdp: answerData.sdp
                };
                await peerConnection.setRemoteDescription(answer);

                addMessage("Connection established with OpenAI");
                return true;
            } catch (error) {
                console.error("Connection Error:", error);
                addMessage(`Connection error: ${error.message}`, true);
                endCall();
                return false;
            }
        }

        function endCall() {
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            if (audioStreamTrack) {
                audioStreamTrack.stop();
                audioStreamTrack = null;
            }
            
            if (peerConnection) {
                peerConnection.close();
                peerConnection = null;
            }
            
            callButton.classList.remove('active');
            callStatus.style.display = "none";
            speechBubble.style.display = 'none';
            isCallActive = false;
            
            if (audioElement) {
                audioElement.srcObject = null;
                audioElement.remove();
                audioElement = null;
            }
            
            addMessage("Call ended");
        }

        async function toggleCall() {
            if (isCallActive) {
                endCall();
            } else {
                callButton.classList.add('active');
                isCallActive = await startConnection();
                if (!isCallActive) {
                    callButton.classList.remove('active');
                }
            }
        }

        callButton.addEventListener('click', toggleCall);

        window.addEventListener('beforeunload', () => {
            if (peerConnection) peerConnection.close();
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
            }
            if (audioStreamTrack) {
                audioStreamTrack.stop();
            }
            if (audioElement) {
                audioElement.srcObject = null;
                audioElement.remove();
            }
        });
        
        document.body.addEventListener('click', () => {
            if (audioContext && audioContext.state === 'suspended') {
                audioContext.resume().then(() => {
                    console.log("Audio context resumed after user interaction.");
                }).catch(error => {
                    console.error("Error resuming audio context:", error);
                    addMessage("Error activating audio. Please ensure your browser allows audio playback.", true);
                });
            }
        }, { once: true });
    </script>
</body>
</html>
