<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>OpenAI Voice Assistant</title>
  <style>
    /* [Styles remain unchanged from the original code] */
  </style>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
</head>
<body>
  <div class="container">
    <div class="phone">
      <div class="phone-screen">
        <div class="phone-status">
          <div id="callStatus" class="call-status">Active Call</div>
        </div>
        <div id="phoneContent" class="phone-content">
          <div class="message-box">
            Ready to connect to OpenAI voice assistant. Press the green button to start.
          </div>
        </div>
        <div class="phone-bottom">
          <div id="callButton" class="call-button">
            <i class="fas fa-phone"></i>
          </div>
        </div>
      </div>
    </div>
    <div id="speechBubble" class="speech-bubble">
      <p>OpenAI responses will appear here once connected.</p>
    </div>
  </div>
  
  <script src="https://unpkg.com/parse/dist/parse.min.js"></script>
  <script>
    Parse.initialize("YOUR_APP_ID", "YOUR_JAVASCRIPT_KEY");
    Parse.serverURL = "YOUR_PARSE_SERVER_URL/";

    let peerConnection = null;
    let dataChannel = null;
    let isCallActive = false;
    let sessionId = null;
    let audioContext = null;
    let mediaStream = null;
    let audioElement = null;
    let isAudioContextResumed = false;
    let clientSecret = null;

    const callButton = document.getElementById('callButton');
    const phoneContent = document.getElementById('phoneContent');
    const callStatus = document.getElementById('callStatus');
    const speechBubble = document.getElementById('speechBubble');

    // Add a message to the phone content
    function addMessage(message, isError = false) {
      const messageBox = document.createElement('div');
      messageBox.className = 'message-box';
      if (isError) messageBox.classList.add('error');
      messageBox.textContent = message;
      phoneContent.appendChild(messageBox);
      phoneContent.scrollTop = phoneContent.scrollHeight;
    }

    // Update speech bubble with AI response
    function updateSpeechBubble(text) {
      speechBubble.innerHTML = `<p>${text}</p>`;
      speechBubble.style.display = 'block';
    }

    // Initialize WebRTC
    function initializeWebRTC(iceServers) {
      try {
        const configuration = { iceServers };
        peerConnection = new RTCPeerConnection(configuration);
        
        peerConnection.onicecandidate = async (event) => {
          if (event.candidate) {
            console.log("ICE Candidate:", event.candidate);
            try {
              await sendSignalingData(sessionId, {
                type: "candidate",
                candidate: event.candidate.candidate
              });
            } catch (error) {
              console.error("Signaling Error:", error);
              addMessage(`Signaling error: ${error.message}`, true);
            }
          }
        };

        peerConnection.onconnectionstatechange = () => {
          console.log("Connection state:", peerConnection.connectionState);
          if (peerConnection.connectionState === "connected") {
            callStatus.style.display = "block";
            addMessage("Connection established successfully!");
            speechBubble.style.display = 'block';
            
            if (audioContext && audioContext.state === 'suspended') {
              audioContext.resume().then(() => {
                console.log("Audio context resumed");
                isAudioContextResumed = true;
              }).catch(error => {
                console.error("Failed to resume audio context:", error);
              });
            }
          } else if (peerConnection.connectionState === "disconnected" || 
                     peerConnection.connectionState === "failed") {
            endCall();
            addMessage("Connection lost or failed", true);
          }
        };

        peerConnection.ontrack = (event) => {
          console.log("Audio Track received:", event.track);
          
          if (!audioElement) {
            audioElement = document.createElement("audio");
            audioElement.autoplay = true;
            audioElement.volume = 1.0;
            audioElement.controls = true; // Added for debugging
            document.body.appendChild(audioElement);
          }
          
          audioElement.srcObject = event.streams[0];
          
          if (audioContext && audioContext.state === 'suspended' && !isAudioContextResumed) {
            addMessage("Audio context is suspended. Please click anywhere to activate audio.");
            document.body.addEventListener('click', () => {
              audioContext.resume().then(() => {
                console.log("Audio context resumed");
                isAudioContextResumed = true;
              }).catch(error => {
                console.error("Failed to resume audio context:", error);
              });
            }, { once: true });
          }
        };

        dataChannel = peerConnection.createDataChannel("oaiControlEvents");
        dataChannel.onopen = () => {
          console.log("Data channel opened");
          addMessage("Communication channel opened");
          
          sendResponseCreate();
        };
        
        dataChannel.onmessage = (e) => {
          try {
            const event = JSON.parse(e.data);
            console.log("OpenAI Realtime Event:", event);
            
            if (event.type === "transcript") {
              updateSpeechBubble(event.text);
            } else if (event.type === "error") {
              addMessage(`OpenAI Error: ${event.text}`, true);
            } else if (event.type === "status") {
              addMessage(`Status: ${event.text}`);
            } else if (event.type === "response") {
              if (event.output && event.output[0] && event.output[0].audio) {
                const audioUrl = event.output[0].audio.url;
                if (audioElement) {
                  audioElement.src = audioUrl;
                  audioElement.play().catch(error => {
                    console.error("Error playing audio:", error);
                    addMessage("Failed to play audio response", true);
                  });
                }
              } else {
                console.error("Unexpected response format:", event);
                addMessage("Received unexpected response format from OpenAI", true);
              }
            } else if (event.type === "response.done") {
              if (event.response.status === "failed") {
                addMessage(`Response failed: ${event.response.status_details.error.message}`, true);
              } else {
                addMessage("Response completed successfully");
              }
            } else {
              console.log("Unhandled event type:", event.type);
            }
          } catch (error) {
            console.error("Error parsing message:", error);
            addMessage(`Error parsing message: ${error.message}`, true);
          }
        };

        return peerConnection;
      } catch (error) {
        console.error("WebRTC initialization error:", error);
        addMessage(`WebRTC initialization error: ${error.message}`, true);
        throw error;
      }
    }

    // Function to send response.create message
    function sendResponseCreate() {
      if (dataChannel && dataChannel.readyState === "open") {
        const message = {
          type: "response.create",
          response: {
            model: "gpt-4o-mini-realtime-preview-2024-12-17",
            voice: "alloy",
            parameters: {
              temperature: 0.7,
              max_tokens: 500
            }
          }
        };
        dataChannel.send(JSON.stringify(message));
      } else {
        console.error("Data channel not open");
      }
    }

    // Start user media (microphone)
    async function startUserMedia() {
      try {
        mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true, video: false });
        addMessage("Microphone access granted");
        
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        
        const track = mediaStream.getAudioTracks()[0];
        peerConnection.addTrack(track, mediaStream);
        
        return mediaStream;
      } catch (error) {
        console.error("Media access error:", error);
        addMessage(`Microphone access error: ${error.message}`, true);
        throw error;
      }
    }

    // Start connection
    async function startConnection() {
      try {
        addMessage("Initializing connection...");
        
        const response = await Parse.Cloud.run("createEphemeralSession", {
          model: "gpt-4o-mini-realtime-preview-2024-12-17",
          voice: "alloy"
        });

        if (!response || typeof response !== 'object' || !response.sessionId || !response.clientSecret) {
          addMessage("Response dump: " + JSON.stringify(response), true);
          throw new Error("Invalid server response: missing required fields");
        }
        
        sessionId = response.sessionId;
        clientSecret = response.clientSecret;

        if (!response.iceServers || !Array.isArray(response.iceServers)) {
          response.iceServers = [
            {
              urls: "stun:stun.relay.metered.ca:80",
            },
            {
              urls: "turn:global.relay.metered.ca:80",
              username: "dabe0db86808a99aa36c7309",
              credential: "n7sdOC9KUqBloUJ4",
            }
          ];
        }
        
        initializeWebRTC(response.iceServers);

        await startUserMedia();

        await handleSignaling(response.sessionId, response.clientSecret);
        
        return true;
      } catch (error) {
        console.error("Connection Error:", error);
        addMessage(`Connection error: ${error.message}`, true);
        endCall();
        return false;
      }
    }

    // Signaling logic
    async function handleSignaling(sessionId, clientSecret) {
      if (!peerConnection) throw new Error("WebRTC not initialized");

      try {
        const offer = await peerConnection.createOffer({
          offerToReceiveAudio: true,
          offerToReceiveVideo: false
        });
        await peerConnection.setLocalDescription(offer);
        
        await Parse.Cloud.run("sendSignalingData", {
          sessionId: sessionId,
          data: { type: "offer", sdp: offer.sdp }
        });
        
        const sdpResponse = await fetch(`https://api.openai.com/v1/realtime?model=gpt-4o-mini-realtime-preview-2024-12-17`, {
          method: "POST",
          body: offer.sdp,
          headers: {
            Authorization: `Bearer ${clientSecret}`,
            "Content-Type": "application/sdp"
          }
        });

        if (!sdpResponse.ok) {
          const errorText = await sdpResponse.text();
          let errorMessage = "Unknown error";
          try {
            const errorData = JSON.parse(errorText);
            errorMessage = errorData.error?.message || "Unknown error";
          } catch (e) {
            errorMessage = errorText || "Unknown error";
          }
          throw new Error(`OpenAI Error: ${errorMessage}`);
        }

        const answerSdp = await sdpResponse.text();
        await peerConnection.setRemoteDescription({
          type: "answer",
          sdp: answerSdp
        });

        addMessage("Connection established with OpenAI");
        return true;
      } catch (error) {
        console.error("Signaling Error:", error);
        addMessage(`Signaling error: ${error.message}`, true);
        throw error;
      }
    }

    // Helper function to send signaling data
    async function sendSignalingData(sessionId, data) {
      try {
        await Parse.Cloud.run("sendSignalingData", {
          sessionId: sessionId,
          data: data
        });
      } catch (error) {
        console.error("Signaling Error:", error);
        throw error;
      }
    }

    // End call function
    function endCall() {
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
        mediaStream = null;
      }
      
      if (peerConnection) {
        peerConnection.close();
        peerConnection = null;
      }
      
      callButton.classList.remove('active');
      callStatus.style.display = "none";
      speechBubble.style.display = 'none';
      isCallActive = false;
      
      if (audioElement) {
        audioElement.srcObject = null;
        audioElement.remove();
        audioElement = null;
      }
      
      addMessage("Call ended");
    }

    // Toggle call
    async function toggleCall() {
      if (isCallActive) {
        endCall();
      } else {
        callButton.classList.add('active');
        isCallActive = await startConnection();
        if (!isCallActive) {
          callButton.classList.remove('active');
        }
      }
    }

    // Event listeners
    callButton.addEventListener('click', toggleCall);

    // Cleanup
    window.addEventListener('beforeunload', () => {
      if (peerConnection) peerConnection.close();
      if (mediaStream) {
        mediaStream.getTracks().forEach(track => track.stop());
      }
      if (audioElement) {
        audioElement.srcObject = null;
        audioElement.remove();
      }
    });

    // Handle audio context state
    if (audioContext && audioContext.state === 'suspended') {
      addMessage("Audio context is suspended. Please click anywhere to activate audio.");
      document.body.addEventListener('click', () => {
        audioContext.resume().then(() => {
          console.log("Audio context resumed");
          isAudioContextResumed = true;
        }).catch(error => {
          console.error("Failed to resume audio context:", error);
        });
      }, { once: true });
    }
  </script>
</body>
</html>
