<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Voice Assistant</title>
  <style>
    body { font-family: Arial; margin: 0; padding: 20px; background: #f0f2f5; }
    .container { max-width: 600px; margin: auto; background: white; padding: 20px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
    #micButton { background: #10B981; width: 80px; height: 80px; border-radius: 50%; border: none; cursor: pointer; margin: 20px auto; display: block; }
    #status { color: #059669; text-align: center; display: none; }
    #logs { background: #1f2937; color: white; padding: 15px; border-radius: 8px; font-family: monospace; height: 200px; overflow-y: auto; }
  </style>
</head>
<body>
  <div class="container">
    <div id="status">● LIVE</div>
    <div id="logs"></div>
    <button id="micButton" onclick="toggleConnection()">
      <i class="fas fa-microphone" style="font-size: 24px;"></i>
    </button>
    <audio id="audioOutput" controls autoplay style="width: 100%; margin-top: 20px;"></audio>
  </div>

  <script src="https://kit.fontawesome.com/845924f09c.js"></script>
  <script src="https://unpkg.com/parse/dist/parse.min.js"></script>
  <script>
    Parse.initialize("uNEbQKjymSX8qmK9gorQNRbSuaV23eMHriF2Yeoi", "NY08Fa7grB4I1AOOcrjKN2b9w6sWke5GCA8rpxBM"); // Ersetze mit deinen Back4App-Schlüsseln
    Parse.serverURL = "https://parseapi.back4app.com/";

    let peerConnection = null;
    let mediaStream = null;
    let audioContext = null;
    let dc = null;

    function log(message) {
      const logs = document.getElementById('logs');
      logs.textContent += `[${new Date().toLocaleTimeString()}] ${message}\n`;
      logs.scrollTop = logs.scrollHeight;
    }

    // VAD-Prozessor-Code direkt im Script-Tag
    const vadProcessorCode = `
      class VADProcessor extends AudioWorkletProcessor {
        static get parameterDescriptors() {
          return [{ name: 'threshold', defaultValue: 0.1 }];
        }

        constructor() {
          super();
          this.speaking = false;
          this.buffer = new Float32Array(512);
          this.index = 0;
          this.threshold = 0.1;

          this.port.onmessage = (e) => {
            if (e.data.threshold) this.threshold = e.data.threshold;
          };
        }

        process(inputs) {
          const input = inputs[0]?.[0];
          if (!input) return true;

          for (let i = 0; i < input.length; i++) {
            this.buffer[this.index] = input[i];
            this.index = (this.index + 1) % 512;
            if (this.index === 0) this.analyze();
          }
          return true;
        }

        analyze() {
          const threshold = this.parameters?.get('threshold')?.[0] || this.threshold;
          const energy = this.buffer.reduce((sum, val) => sum + val ** 2, 0) / 512;

          if (energy > threshold && !this.speaking) {
            this.speaking = true;
            this.port.postMessage({ type: "speech_started" });
          } else if (energy < threshold * 0.5 && this.speaking) {
            this.speaking = false;
            this.port.postMessage({ type: "speech_stopped" });
          }
        }
      }
      registerProcessor('vad-processor', VADProcessor);
    `;

    // VAD-Prozessor-Modul als Blob erstellen und laden
    async function loadVADProcessor() {
      const blob = new Blob([vadProcessorCode], { type: 'application/javascript' });
      const url = URL.createObjectURL(blob);
      await audioContext.audioWorklet.addModule(url);
    }

    async function initConnection() {
      try {
        log("Initializing connection...");

        const session = await Parse.Cloud.run("createEphemeralSession");

        peerConnection = new RTCPeerConnection({
          iceServers: session.iceServers,
          iceTransportPolicy: "relay"
        });

        mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaStream.getTracks().forEach(track => peerConnection.addTrack(track));

        peerConnection.ontrack = async (event) => {
          const audioElement = document.getElementById('audioOutput');
          audioElement.srcObject = event.streams[0];
          try {
            await audioElement.play();
            log("Audio-Wiedergabe gestartet");
          } catch (err) {
            log("Klicke auf den Audio-Player, um die Wiedergabe zu starten");
            audioElement.muted = true;
            await audioElement.play();
            audioElement.muted = false;
          }
        };

        dc = peerConnection.createDataChannel("oai-events");
        dc.onopen = () => {
          log("Datenkanal geöffnet");
          const responseCreate = {
            type: "response.create",
            response: {
              modalities: ["text", "audio"],
              instructions: "Antworte auf die Eingaben des Benutzers hilfreich und freundlich.",
              voice: "verse",
              output_audio_format: "pcm16"
            }
          };
          dc.send(JSON.stringify(responseCreate));
        };
        dc.onmessage = (event) => {
          const message = JSON.parse(event.data);
          log(`Nachricht empfangen: ${JSON.stringify(message)}`);
          if (message.type === "response.text.delta") {
            log(`Textantwort: ${message.delta}`);
          }
        };

        // AudioWorklet initialisieren
        audioContext = new AudioContext();
        await loadVADProcessor();
        const vadProcessor = new AudioWorkletNode(audioContext, 'vad-processor');

        vadProcessor.port.onmessage = (e) => {
          if (e.data.type === "speech_started") log("Sprache erkannt");
          if (e.data.type === "speech_stopped") log("Sprache beendet");
        };

        const source = audioContext.createMediaStreamSource(mediaStream);
        source.connect(vadProcessor);
        vadProcessor.connect(audioContext.destination);

        const offer = await peerConnection.createOffer();
        await peerConnection.setLocalDescription(offer);

        const answer = await Parse.Cloud.run("forwardToOpenAI", {
          clientSecret: session.clientSecret,
          sdp: offer.sdp
        });

        await peerConnection.setRemoteDescription(
          new RTCSessionDescription({
            type: 'answer',
            sdp: answer.sdp
          })
        );

        document.getElementById('status').style.display = 'block';
        log("Verbindung erfolgreich!");
      } catch (error) {
        log(`Fehler: ${error.message}`);
      }
    }

    async function toggleConnection() {
      if (peerConnection?.connectionState === 'connected') {
        peerConnection.close();
        mediaStream.getTracks().forEach(track => track.stop());
        if (audioContext) await audioContext.close();
        document.getElementById('status').style.display = 'none';
        document.getElementById('audioOutput').srcObject = null;
        log("Verbindung getrennt");
      } else {
        await initConnection();
      }
    }
  </script>
</body>
</html>
