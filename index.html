<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenAI Voice Assistant</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            background-color: #f4f4f4;
            margin: 0;
        }
        .container {
            display: flex;
            align-items: center;
            gap: 20px;
        }
        .phone {
            width: 300px;
            height: 600px;
            background-color: #111;
            border-radius: 30px;
            padding: 20px;
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
            position: relative;
            overflow: hidden;
        }
        .phone-screen {
            background-color: white;
            height: 100%;
            border-radius: 20px;
            overflow: hidden;
            display: flex;
            flex-direction: column;
        }
        .phone-status {
            height: 30px;
            background-color: #f8f8f8;
            border-bottom: 1px solid #ddd;
            display: flex;
            align-items: center;
            padding: 0 15px;
        }
        .call-status {
            font-size: 14px;
            font-weight: bold;
            color: #4CAF50;
            display: none;
        }
        .phone-content {
            flex: 1;
            padding: 15px;
            overflow-y: auto;
            background-color: #f9f9f9;
        }
        .message-box {
            background-color: white;
            border-radius: 10px;
            padding: 10px;
            margin-bottom: 10px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
            word-wrap: break-word;
        }
        .phone-bottom {
            height: 80px;
            background-color: #f8f8f8;
            display: flex;
            justify-content: center;
            align-items: center;
            border-top: 1px solid #ddd;
        }
        .call-button {
            width: 60px;
            height: 60px;
            border-radius: 50%;
            background-color: #4CAF50;
            display: flex;
            justify-content: center;
            align-items: center;
            cursor: pointer;
            transition: background-color 0.3s;
        }
        .call-button.active {
            background-color: #f44336;
        }
        .call-button i {
            color: white;
            font-size: 24px;
        }
        .speech-bubble {
            position: relative;
            background: #ffffff;
            border-radius: 15px;
            padding: 20px;
            width: 250px;
            min-height: 100px;
            max-height: 400px;
            overflow-y: auto;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            display: none;
        }
        .speech-bubble:after {
            content: '';
            position: absolute;
            left: 0;
            top: 30px;
            width: 0;
            height: 0;
            border: 15px solid transparent;
            border-right-color: #ffffff;
            border-left: 0;
            margin-top: -15px;
            margin-left: -15px;
        }
        .error {
            color: #f44336;
            font-weight: bold;
        }
        audio {
            display: none;
        }
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
</head>
<body>
    <div class="container">
        <div class="phone">
            <div class="phone-screen">
                <div class="phone-status">
                    <div id="callStatus" class="call-status">Active Call</div>
                </div>
                <div id="phoneContent" class="phone-content">
                    <div class="message-box">
                        Ready to connect to OpenAI voice assistant. Press the green button to start.
                    </div>
                </div>
                <div class="phone-bottom">
                    <div id="callButton" class="call-button">
                        <i class="fas fa-phone"></i>
                    </div>
                </div>
            </div>
        </div>
        <div id="speechBubble" class="speech-bubble">
            <p>OpenAI responses will appear here once connected.</p>
        </div>
    </div>
     <script src="https://unpkg.com/parse/dist/parse.min.js"></script>
    <script>
        // Initialize Parse
        Parse.initialize("uNEbQKjymSX8qmK9gorQNRbSuaV23eMHriF2Yeoi", "NY08Fa7grB4I1AOOcrjKN2b9w6sWke5GCA8rpxBM");
        Parse.serverURL = "https://parseapi.back4app.com";

        // Application state
        const appState = {
            peerConnection: null,
            dataChannel: null,
            isCallActive: false,
            mediaStream: null,
            audioContext: null,
            audioElement: null,
            currentSession: null
        };

        // DOM elements
        const ui = {
            callButton: document.getElementById('callButton'),
            phoneContent: document.getElementById('phoneContent'),
            callStatus: document.getElementById('callStatus'),
            speechBubble: document.getElementById('speechBubble')
        };

        // Utility functions
        function displayMessage(text, isError = false) {
            const messageElement = document.createElement('div');
            messageElement.className = `message-box ${isError ? 'error' : ''}`;
            messageElement.textContent = text;
            ui.phoneContent.appendChild(messageElement);
            ui.phoneContent.scrollTop = ui.phoneContent.scrollHeight;
        }

        function updateSpeechBubble(content) {
            ui.speechBubble.innerHTML = `<p>${content}</p>`;
            ui.speechBubble.style.display = 'block';
        }

        // WebRTC setup
        async function setupWebRTCConnection(iceServers, clientSecret) {
            try {
                // Initialize PeerConnection
                appState.peerConnection = new RTCPeerConnection({ iceServers });
                
                // Setup ICE candidate handling
                appState.peerConnection.onicecandidate = (event) => {
                    if (event.candidate) {
                        console.debug("New ICE candidate:", event.candidate);
                    }
                };

                // Connection state handling
                appState.peerConnection.onconnectionstatechange = () => {
                    console.log("Connection state:", appState.peerConnection.connectionState);
                    switch(appState.peerConnection.connectionState) {
                        case "connected":
                            ui.callStatus.style.display = "block";
                            displayMessage("WebRTC connection established");
                            break;
                        case "disconnected":
                        case "failed":
                            endCall();
                            displayMessage("Connection lost", true);
                            break;
                    }
                };

                // Data channel setup
                appState.dataChannel = appState.peerConnection.createDataChannel("oai-events");
                appState.dataChannel.onopen = () => {
                    displayMessage("Data channel ready");
                    console.log("Data channel opened");
                };
                
                appState.dataChannel.onmessage = (event) => {
                    try {
                        const data = JSON.parse(event.data);
                        console.log("OpenAI Event:", data);
                        
                        if (data.type === "transcript") {
                            updateSpeechBubble(data.text);
                        } else if (data.type === "error") {
                            handleApiError(data);
                        }
                    } catch (error) {
                        console.error("Error parsing message:", error);
                    }
                };

                // Audio track handling
                appState.peerConnection.ontrack = (event) => {
                    if (!appState.audioElement) {
                        appState.audioElement = document.createElement("audio");
                        appState.audioElement.autoplay = true;
                        document.body.appendChild(appState.audioElement);
                    }
                    appState.audioElement.srcObject = event.streams[0];
                };

                // Get user media
                appState.mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                appState.audioContext = new (window.AudioContext || window.webkitAudioContext)();
                
                // Add audio track to connection
                appState.mediaStream.getAudioTracks().forEach(track => {
                    appState.peerConnection.addTrack(track, appState.mediaStream);
                });

                // Create and set local description
                const offer = await appState.peerConnection.createOffer({
                    offerToReceiveAudio: true,
                    offerToReceiveVideo: false
                });
                await appState.peerConnection.setLocalDescription(offer);

                // Connect to OpenAI with the ephemeral key
                const response = await fetch("https://api.openai.com/v1/realtime/sessions", {
                    method: "POST",
                    headers: {
                        "Authorization": `Bearer ${clientSecret}`,
                        "Content-Type": "application/json"
                    },
                    body: JSON.stringify({
                        sdp: offer.sdp,
                        model: "gpt-4o-realtime-preview-2024-12-17"
                    })
                });

                if (!response.ok) {
                    const errorData = await response.json();
                    throw new Error(errorData.error?.message || "Failed to establish OpenAI connection");
                }

                const answerData = await response.json();
                await appState.peerConnection.setRemoteDescription({
                    type: "answer",
                    sdp: answerData.sdp
                });

                displayMessage("Successfully connected to OpenAI");
                return true;

            } catch (error) {
                console.error("WebRTC Setup Error:", error);
                throw error;
            }
        }

        function handleApiError(errorData) {
            const errorMessage = errorData.error?.message || 
                               errorData.message || 
                               "Unknown API error";
            displayMessage(`Error: ${errorMessage}`, true);
            endCall();
        }

        // Cleanup resources
        function endCall() {
            // Stop all media tracks
            if (appState.mediaStream) {
                appState.mediaStream.getTracks().forEach(track => track.stop());
                appState.mediaStream = null;
            }
            
            // Close peer connection
            if (appState.peerConnection) {
                appState.peerConnection.close();
                appState.peerConnection = null;
            }
            
            // Clean up audio element
            if (appState.audioElement) {
                appState.audioElement.pause();
                appState.audioElement.srcObject = null;
                appState.audioElement.remove();
                appState.audioElement = null;
            }
            
            // Reset UI state
            ui.callButton.classList.remove('active');
            ui.callStatus.style.display = "none";
            ui.speechBubble.style.display = 'none';
            appState.isCallActive = false;
            
            displayMessage("Call ended");
        }

        // Main connection handler
        async function toggleCall() {
            if (appState.isCallActive) {
                endCall();
                return;
            }

            try {
                ui.callButton.classList.add('active');
                displayMessage("Starting connection...");
                
                // Get session data from backend
                const sessionData = await Parse.Cloud.run("createEphemeralSession", {
                    model: "gpt-4o-realtime-preview-2024-12-17",
                    voice: "alloy"
                });

                if (!sessionData?.success || !sessionData.clientSecret) {
                    throw new Error("Invalid session data from server");
                }

                // Store session reference
                appState.currentSession = sessionData;
                appState.isCallActive = true;

                // Initialize WebRTC connection
                await setupWebRTCConnection(sessionData.iceServers, sessionData.clientSecret);

            } catch (error) {
                console.error("Connection Error:", error);
                displayMessage(`Connection failed: ${error.message}`, true);
                endCall();
            }
        }

        // Event listeners
        ui.callButton.addEventListener('click', toggleCall);
        window.addEventListener('beforeunload', endCall);
        
        // Handle audio context resume
        document.addEventListener('click', () => {
            if (appState.audioContext?.state === 'suspended') {
                appState.audioContext.resume().then(() => {
                    console.log("Audio context resumed");
                });
            }
        }, { once: true });
    </script>
</body>
</html>
