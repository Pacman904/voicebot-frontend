<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenAI Voice Assistant</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            height: 100vh;
            background-color: #f4f4f4;
            margin: 0;
        }
        .container {
            display: flex;
            align-items: center;
            gap: 20px;
        }
        .phone {
            width: 300px;
            height: 600px;
            background-color: #111;
            border-radius: 30px;
            padding: 20px;
            box-shadow: 0 10px 20px rgba(0, 0, 0, 0.3);
            position: relative;
            overflow: hidden;
        }
        .phone-screen {
            background-color: white;
            height: 100%;
            border-radius: 20px;
            overflow: hidden;
            display: flex;
            flex-direction: column;
        }
        .phone-status {
            height: 30px;
            background-color: #f8f8f8;
            border-bottom: 1px solid #ddd;
            display: flex;
            align-items: center;
            padding: 0 15px;
        }
        .call-status {
            font-size: 14px;
            font-weight: bold;
            color: #4CAF50;
            display: none;
        }
        .phone-content {
            flex: 1;
            padding: 15px;
            overflow-y: auto;
            background-color: #f9f9f9;
        }
        .message-box {
            background-color: white;
            border-radius: 10px;
            padding: 10px;
            margin-bottom: 10px;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
            word-wrap: break-word;
        }
        .phone-bottom {
            height: 80px;
            background-color: #f8f8f8;
            display: flex;
            justify-content: center;
            align-items: center;
            border-top: 1px solid #ddd;
        }
        .call-button {
            width: 60px;
            height: 60px;
            border-radius: 50%;
            background-color: #4CAF50;
            display: flex;
            justify-content: center;
            align-items: center;
            cursor: pointer;
            transition: background-color 0.3s;
        }
        .call-button.active {
            background-color: #f44336;
        }
        .call-button i {
            color: white;
            font-size: 24px;
        }
        .speech-bubble {
            position: relative;
            background: #ffffff;
            border-radius: 15px;
            padding: 20px;
            width: 250px;
            min-height: 100px;
            max-height: 400px;
            overflow-y: auto;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            display: none;
        }
        .speech-bubble:after {
            content: '';
            position: absolute;
            left: 0;
            top: 30px;
            width: 0;
            height: 0;
            border: 15px solid transparent;
            border-right-color: #ffffff;
            border-left: 0;
            margin-top: -15px;
            margin-left: -15px;
        }
        .error {
            color: #f44336;
            font-weight: bold;
        }
        audio {
            display: none;
        }
    </style>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css">
</head>
<body>
    <div class="container">
        <div class="phone">
            <div class="phone-screen">
                <div class="phone-status">
                    <div id="callStatus" class="call-status">Active Call</div>
                </div>
                <div id="phoneContent" class="phone-content">
                    <div class="message-box">
                        Ready to connect to OpenAI voice assistant. Press the green button to start.
                    </div>
                </div>
                <div class="phone-bottom">
                    <div id="callButton" class="call-button">
                        <i class="fas fa-phone"></i>
                    </div>
                </div>
            </div>
        </div>
        <div id="speechBubble" class="speech-bubble">
            <p>OpenAI responses will appear here once connected.</p>
        </div>
    </div>
      <script src="https://unpkg.com/parse/dist/parse.min.js"></script>
    <script>
        // Configuration
        const CONFIG = {
            parseAppId: "uNEbQKjymSX8qmK9gorQNRbSuaV23eMHriF2Yeoi",
            parseJSKey: "NY08Fa7grB4I1AOOcrjKN2b9w6sWke5GCA8rpxBM",
            parseServerURL: "https://parseapi.back4app.com",
            openAIModel: "gpt-4o-mini-realtime-preview-2024-12-17",
            openAIVoice: "alloy"
        };

        // Initialize Parse
        Parse.initialize(CONFIG.parseAppId, CONFIG.parseJSKey);
        Parse.serverURL = CONFIG.parseServerURL;

        // Application state
        const state = {
            peerConnection: null,
            dataChannel: null,
            isCallActive: false,
            mediaStream: null,
            audioContext: null,
            audioElement: null,
            currentSession: null
        };

        // DOM elements
        const elements = {
            callButton: document.getElementById('callButton'),
            phoneContent: document.getElementById('phoneContent'),
            callStatus: document.getElementById('callStatus'),
            speechBubble: document.getElementById('speechBubble')
        };

        // Utility functions
        function displayMessage(text, isError = false) {
            console.log(`[UI] ${isError ? 'Error:' : 'Message:'} ${text}`);
            const messageElement = document.createElement('div');
            messageElement.className = `message-box ${isError ? 'error' : ''}`;
            messageElement.textContent = text;
            elements.phoneContent.appendChild(messageElement);
            elements.phoneContent.scrollTop = elements.phoneContent.scrollHeight;
        }

        function updateSpeechBubble(content) {
            console.log(`[UI] Updating speech bubble: ${content}`);
            elements.speechBubble.innerHTML = `<p>${content}</p>`;
            elements.speechBubble.style.display = 'block';
        }

        // WebRTC Connection Management
        async function initializeWebRTC(iceServers) {
            try {
                console.log("[WebRTC] Initializing with ICE servers:", iceServers);
                state.peerConnection = new RTCPeerConnection({ iceServers });
                
                // ICE Candidate handling
                state.peerConnection.onicecandidate = (event) => {
                    if (event.candidate) {
                        console.debug("[WebRTC] New ICE candidate:", event.candidate);
                    }
                };

                // Connection state handling
                state.peerConnection.onconnectionstatechange = () => {
                    console.log("[WebRTC] Connection state:", state.peerConnection.connectionState);
                    switch(state.peerConnection.connectionState) {
                        case "connected":
                            elements.callStatus.style.display = "block";
                            displayMessage("Connection established!");
                            break;
                        case "disconnected":
                        case "failed":
                            endCall();
                            displayMessage("Connection lost", true);
                            break;
                    }
                };

                // Data channel setup
                state.dataChannel = state.peerConnection.createDataChannel("oai-events");
                state.dataChannel.onopen = () => {
                    displayMessage("Data channel ready");
                    console.log("[WebRTC] Data channel opened");
                };
                
                state.dataChannel.onmessage = (event) => {
                    try {
                        const data = JSON.parse(event.data);
                        console.log("[WebRTC] Received event:", data);
                        
                        if (data.type === "transcript") {
                            updateSpeechBubble(data.text);
                        } else if (data.type === "error") {
                            handleOpenAIError(data);
                        }
                    } catch (error) {
                        console.error("[WebRTC] Error parsing message:", error);
                    }
                };

                // Audio track handling
                state.peerConnection.ontrack = (event) => {
                    console.log("[WebRTC] Received audio track");
                    if (!state.audioElement) {
                        state.audioElement = document.createElement("audio");
                        state.audioElement.autoplay = true;
                        document.body.appendChild(state.audioElement);
                    }
                    state.audioElement.srcObject = event.streams[0];
                };

                return state.peerConnection;
            } catch (error) {
                console.error("[WebRTC] Initialization error:", error);
                throw error;
            }
        }

        async function setupMedia() {
            try {
                console.log("[Media] Requesting user media");
                state.mediaStream = await navigator.mediaDevices.getUserMedia({ 
                    audio: true, 
                    video: false 
                });
                
                state.audioContext = new (window.AudioContext || window.webkitAudioContext)();
                
                // Add audio track to connection
                state.mediaStream.getAudioTracks().forEach(track => {
                    console.log("[Media] Adding audio track to connection");
                    state.peerConnection.addTrack(track, state.mediaStream);
                });

                displayMessage("Microphone ready");
                return true;
            } catch (error) {
                console.error("[Media] Error:", error);
                displayMessage(`Media Error: ${error.message}`, true);
                throw error;
            }
        }

        async function establishWebRTCConnection(clientSecret) {
            try {
                console.log("[WebRTC] Creating SDP offer");
                const offer = await state.peerConnection.createOffer({
                    offerToReceiveAudio: true,
                    offerToReceiveVideo: false
                });
                await state.peerConnection.setLocalDescription(offer);

                console.log("[WebRTC] Connecting with client secret");
                
                // The key fix: Using the correct endpoint format for WebRTC connection
                const response = await fetch(`https://api.openai.com/v1/realtime/sessions/${state.currentSession.sessionId}`, {
                    method: "POST",
                    headers: {
                        "Authorization": `Bearer ${clientSecret}`,
                        "Content-Type": "application/json"
                    },
                    body: JSON.stringify({
                        sdp: offer.sdp,
                        type: "offer"
                    })
                });

                if (!response.ok) {
                    const errorData = await response.json().catch(() => ({}));
                    console.error("[WebRTC] Connection Error:", errorData);
                    throw new Error(errorData.error?.message || "WebRTC connection failed");
                }

                const answerData = await response.json();
                console.log("[WebRTC] Received answer:", answerData);
                
                await state.peerConnection.setRemoteDescription({
                    type: "answer",
                    sdp: answerData.sdp
                });

                displayMessage("WebRTC connection established");
                return true;
            } catch (error) {
                console.error("[WebRTC] Connection error:", error);
                throw error;
            }
        }

        function handleOpenAIError(errorData) {
            const errorMessage = errorData.error?.message || 
                               errorData.message || 
                               "Unknown OpenAI error";
            console.error("[OpenAI] Error:", errorMessage);
            displayMessage(`Error: ${errorMessage}`, true);
            endCall();
        }

        // Main call control functions
        async function startCall() {
            try {
                elements.callButton.classList.add('active');
                displayMessage("Initializing connection...");
                
                // Get session data from backend
                console.log("[App] Requesting ephemeral session");
                const sessionData = await Parse.Cloud.run("createEphemeralSession", {
                    model: CONFIG.openAIModel,
                    voice: CONFIG.openAIVoice
                });

                console.log("[App] Received session data:", sessionData);

                if (!sessionData?.success || !sessionData.clientSecret) {
                    throw new Error("Invalid session data from server");
                }

                // Store session reference
                state.currentSession = sessionData;
                state.isCallActive = true;

                // Initialize WebRTC
                await initializeWebRTC(sessionData.iceServers);
                await setupMedia();
                await establishWebRTCConnection(sessionData.clientSecret);

            } catch (error) {
                console.error("[App] Connection error:", error);
                displayMessage(`Connection failed: ${error.message}`, true);
                endCall();
            }
        }

        function endCall() {
            console.log("[App] Ending call");
            
            // Clean up media
            if (state.mediaStream) {
                state.mediaStream.getTracks().forEach(track => track.stop());
                state.mediaStream = null;
            }
            
            // Clean up WebRTC
            if (state.peerConnection) {
                state.peerConnection.close();
                state.peerConnection = null;
            }
            
            // Clean up audio
            if (state.audioElement) {
                state.audioElement.pause();
                state.audioElement.srcObject = null;
                state.audioElement.remove();
                state.audioElement = null;
            }
            
            // Reset UI
            elements.callButton.classList.remove('active');
            elements.callStatus.style.display = "none";
            elements.speechBubble.style.display = 'none';
            state.isCallActive = false;
            
            displayMessage("Call ended");
        }

        function toggleCall() {
            if (state.isCallActive) {
                endCall();
            } else {
                startCall();
            }
        }

        // Event listeners
        elements.callButton.addEventListener('click', toggleCall);
        window.addEventListener('beforeunload', endCall);
        
        // Handle audio context resume
        document.addEventListener('click', () => {
            if (state.audioContext?.state === 'suspended') {
                state.audioContext.resume().then(() => {
                    console.log("[Audio] Context resumed");
                });
            }
        }, { once: true });

        console.log("[App] Initialization complete");
    </script>
</body>
</html>
