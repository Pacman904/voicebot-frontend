<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>WebRTC with OpenAI</title>
  <style>
    body {
      font-family: Arial, sans-serif;
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100vh;
      background-color: #f4f4f4;
    }
    .container {
      text-align: center;
      background-color: white;
      padding: 20px;
      border-radius: 10px;
      box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    }
    button {
      padding: 10px 20px;
      font-size: 16px;
      cursor: pointer;
      background-color: #007bff;
      color: white;
      border: none;
      border-radius: 5px;
    }
    button:hover {
      background-color: #0056b3;
    }
    #onAir {
      margin-top: 20px;
      font-size: 24px;
      color: green;
      display: none;
    }
    #output {
      margin-top: 20px;
      font-size: 16px;
      color: #333;
    }
    #error {
      margin-top: 10px;
      font-size: 14px;
      color: red;
      display: none;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Voicebot</h1>
    <button id="startButton">Start Connection</button>
    <div id="onAir">CONNECTED</div>
    <div id="output"></div>
    <div id="error"></div>
  </div>

  <script src="https://unpkg.com/parse/dist/parse.min.js"></script>
  <script>
    Parse.initialize("uNEbQKjymSX8qmK9gorQNRbSuaV23eMHriF2Yeoi", "NY08Fa7grB4I1AOOcrjKN2b9w6sWke5GCA8rpxBM");
    Parse.serverURL = "https://parseapi.back4app.com/";

    let peerConnection;
    let ws;
    let mediaRecorder;
    let audioChunks = [];
    let sessionId;
    let openaiSessionId;

    const startButton = document.getElementById("startButton");
    const onAirElement = document.getElementById("onAir");
    const outputElement = document.getElementById("output");
    const errorElement = document.getElementById("error");

    startButton.addEventListener("click", async () => {
      if (startButton.textContent === "Stop Connection") {
        // Verbindung beenden
        if (ws) ws.close();
        if (peerConnection) peerConnection.close();
        if (mediaRecorder) mediaRecorder.stop();
        startButton.textContent = "Start Connection";
        onAirElement.style.display = "none";
        outputElement.textContent = "";
        return;
      }

      try {
        startButton.textContent = "Connecting...";
        // Session vom Backend abrufen
        const response = await Parse.Cloud.run("createEphemeralSession", {
          model: "gpt-4o-realtime-preview-2024-12-17",
          voice: "verse",
        });
        sessionId = response.sessionId;
        openaiSessionId = response.openaiSessionId;
        const iceServers = response.iceServers;

        // WebRTC-Verbindung initialisieren
        peerConnection = new RTCPeerConnection({ iceServers });
        peerConnection.onicecandidate = (event) => {
          if (event.candidate) {
            console.log("ICE Candidate:", event.candidate);
          }
        };
        peerConnection.onconnectionstatechange = () => {
          console.log("Connection state:", peerConnection.connectionState);
          if (peerConnection.connectionState === "connected") {
            onAirElement.style.display = "block";
            startButton.textContent = "Stop Connection";
          }
        };
        peerConnection.ontrack = (event) => {
          const audioEl = document.createElement("audio");
          audioEl.autoplay = true;
          audioEl.srcObject = event.streams[0];
          document.body.appendChild(audioEl);
        };

        // WebSocket-Verbindung zur OpenAI Realtime API
        ws = new WebSocket(`wss://api.openai.com/v1/realtime/${openaiSessionId}`, {
          headers: { "OpenAI-Beta": "realtime=v1" },
        });

        ws.onopen = () => {
          console.log("WebSocket-Verbindung geÃ¶ffnet");
          ws.send(JSON.stringify({
            type: "session.update",
            session: {
              modalities: ["audio", "text"],
              instructions: "Du bist ein hilfreicher Voicebot.",
            },
          }));
        };

        ws.onmessage = (event) => {
          const data = JSON.parse(event.data);
          if (data.type === "response.text") {
            outputElement.textContent += `${data.text}\n`;
          } else if (data.type === "response.audio") {
            const audioBlob = new Blob([Buffer.from(data.audio, "base64")], { type: "audio/webm" });
            const audioUrl = URL.createObjectURL(audioBlob);
            new Audio(audioUrl).play();
          }
        };

        ws.onerror = (error) => {
          errorElement.style.display = "block";
          errorElement.textContent = "Verbindungsfehler zur OpenAI API";
          console.error("WebSocket-Fehler:", error);
        };

        // Audio vom Mikrofon aufnehmen
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        peerConnection.addTrack(stream.getAudioTracks()[0], stream);

        mediaRecorder = new MediaRecorder(stream);
        mediaRecorder.ondataavailable = (event) => {
          if (ws.readyState === WebSocket.OPEN) {
            ws.send(event.data); // Audio an OpenAI senden
          }
        };
        mediaRecorder.start(1000); // Audio alle 1 Sekunde senden

      } catch (error) {
        errorElement.style.display = "block";
        errorElement.textContent = `Fehler: ${error.message}`;
        console.error("Fehler beim Verbindungsaufbau:", error);
        startButton.textContent = "Start Connection";
        onAirElement.style.display = "none";
      }
    });
  </script>
</body>
</html>
